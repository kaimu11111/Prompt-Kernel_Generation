
You are a CUDA‑kernel optimisation specialist.
Target GPU: **NVIDIA Quadro RTX 6000 (Turing)**
• GPU Memory: 24GB GDDR6 with ECC
• Memory Bandwidth: 624 GB/s
• FP32 TFLOPS: 16.3
• TF32 Tensor Core TFLOPS: — (Turing 不支持 TF32)
• FP16 Tensor Core TFLOPS: 261 (522 with sparsity)*
• FP8 Tensor Core TFLOPS: — (不支持 FP8)
• INT8 Tensor Core TOPS: 261 (522 with sparsity)*
• Register File Size: 64K 32-bit registers per SM
• Maximum number of registers per thread: 255
• Maximum number of thread blocks per SM: 32
• Shared memory capacity per SM: 64 KB
• Maximum shared memory per thread block: 64 KB

Task
----
Generate **hand‑written CUDA kernels** that replace *all* PyTorch operator(s)
inside the original `class Model` (shown later).  You may fuse multiple
operators into a single kernel if that yields better performance.  Leave any
non‑replaced parts of the model unchanged.

OUTPUT RULES (STRICT) ────────────────────────────────────────────────
1. Reply with **one—and only one—fenced Python block**.  No prose.
2. The block must be directly runnable:
       python model_new.py
3. Inside the block, follow **exactly** this order:
   1. Imports – `torch`, `torch.nn`, `load_inline`.
   2. `source` – triple‑quoted CUDA string(s) (kernel + host wrapper).
   3. `cpp_src` – prototypes for *all* kernels you expose.
   4. **One** `load_inline` call per kernel group.
   5. `class ModelNew(nn.Module)` – mirrors original inputs/outputs but calls
      your CUDA kernels.
4. **Do NOT include** testing code, `if __name__ == "__main__"`, or extra prose.

Few‑shot example (reference only – do **not** echo):
**Original**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()

    def forward(self, a, b):
        return a + b


def get_inputs():
    # randomly generate input tensors based on the model architecture
    a = torch.randn(1, 128).cuda()
    b = torch.randn(1, 128).cuda()
    return [a, b]


def get_init_inputs():
    # randomly generate tensors required for initialization based on the model architecture
    return []
```
**Optimised**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for element-wise addition
source = """
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void elementwise_add_kernel(const float* a, const float* b, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        out[idx] = a[idx] + b[idx];
    }
}

torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b) {
    auto size = a.numel();
    auto out = torch::zeros_like(a);

    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;

    elementwise_add_kernel<<<num_blocks, block_size>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), size);

    return out;
}
"""

cpp_src = (
    "torch::Tensor elementwise_add_cuda(torch::Tensor a, torch::Tensor b);"
)

# Compile the inline CUDA code for element-wise addition
elementwise_add = load_inline(
    name="elementwise_add",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["elementwise_add_cuda"],
    verbose=True,
    extra_cflags=[""],
    extra_ldflags=[""],
)


class ModelNew(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.elementwise_add = elementwise_add

    def forward(self, a, b):
        return self.elementwise_add.elementwise_add_cuda(a, b)
```

Target architecture (to optimise):
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs a ReLU activation.
    """
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies ReLU activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of any shape.

        Returns:
            torch.Tensor: Output tensor with ReLU applied, same shape as input.
        """
        return torch.relu(x)

batch_size = 4096
dim = 200000

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
```

# ---------- Existing kernels (context only) ----------
## Existing kernels
(None yet)


# ---------- Diversity requirement ----------
The kernel you generate **must be meaningfully different** from every existing kernel listed above. Acceptable forms of difference include, but are not limited to:

Do **not** simply adjust constants or reorder lines of an existing kernel.

Now output **only** the complete, runnable `ModelNew` script that satisfies
**ALL OUTPUT RULES (STRICT)** above.
# ==========================================================
# OUTPUT FORMAT – copy verbatim
Return **exactly one** fenced block labelled `python`.  No text before or after.
Use this skeleton (the model must fill it):

```python
# <complete ModelNew code>
```
# ==========================================================
